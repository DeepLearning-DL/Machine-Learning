# Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.
It is more popular algorithm used for classification problems.

# SVM can be implemented in three different ways.

1. Maximum Margin Classifier
2. Classification with Inseparable Classes
3. Kernel Methods

Maximum Margin Classifier
When your data can be completely separated, the linear version of SVMs attempts to maximize the distance from the linear boundary to the closest points
(called the support vectors).

# Classification with Inseparable Classes

Data in the real world is rarely completely separable .So we introduced a new hyper-parameter called C.
The C hyper-parameter determines how flexible we are willing to be with the points that fall on the wrong side of our dividing boundary.
The value of C ranges between 0 and infinity.
When C is large, you are forcing your boundary to have fewer errors than when it is a small value.

Note: when C is too large for a particular set of data, you might not get convergence at all because your data cannot be separated with the small number
of errors allotted with such a large value of C.

# Kernels
 Kernels in SVMs allow us the ability to separate data when the boundary between them is nonlinear.

There are two types of kernels:

1. polynomial
2. rbf

RBF
Most popular kernel is the rbf kernel (which stands for radial basis function). 
The rbf kernel allows you the opportunity to classify points that seem hard to separate in any space. 
This is a density based approach that looks at the closeness of points to one another.
This introduces another hyper-parameter gamma. When gamma is large, the outcome is similar to having a large value of C,
that is your algorithm will attempt to classify every point correctly. 
